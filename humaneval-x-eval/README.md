# HumanEval-X

This README explains how to evaluate a code generation model on HumanEval-X. It uses CodeGeeX that contains the evaluation code. The simplest method is to use the given apptainer image, but other methods are described.

## 1. CodeGeeX installation from Aymara's apptainer (i.e. singularity) or docker image in GitHub's registry

Pull an apptainer image automatically built from a fork of the original CodeGeeX, available on the github docker images registry, if you are working on a cluster without docker like FactoryAI:

```bash
$ singularity -v pull codegeex_latest.sif docker://ghcr.io/aymara/codegeex:main
```

You can also pull instead the Docker image. You can retrieve it with:

```bash
$ docker pull ghcr.io/aymara/codegeex:main
```

## 2. Evaluate CodeGeeX on HumanEval-X

You can evaluate either on your laptop (Section 2.1) or on the FactoryAI cluster (Section 2.2). Any other similar cluster will do the job. Just adapt the script to your situation.

### 2.1. On your laptop

The official evaluation [README](https://github.com/aymara/CodeGeeX/blob/main/codegeex/benchmark/README.md) is available on [CodeGeeX repository](https://github.com/aymara/CodeGeeX).

### 2.2. On FactoryAI

**Step 1 (Optional)** - If you have not pulled the singularity image on FactoryAI at 1, convert the docker image into a singularity image (in local) and send it to FactoryAI

```bash
$ apptainer build codegeex-1.0.sif docker-daemon:codegeex:1.0
```

**Step 2 (Optional)** - Install apptainer on FactoryAI

Singularity is installed on FactoryAI, so you should be able to skip this step. But if it does not work for you for any reason, install Apptainer (the more Free Singularity fork) by following the [official documentation](https://apptainer.org/docs/admin/main/installation.html#install-unprivileged-from-pre-built-binaries). Do not forget add the binary directory in you PATH.

**Step 3** - Launch the evaluation using the provided slurm file (`evaluate-codegeex-on-humanevalx.slurm`)

Edit the environment variables at the top of the script
- `SIF_IMAGE`: path to the HumanEval-X sif image
- `LANGUAGE`: programming language you want to evaluate
- `TEST_FILE`: solutions generated by the model you want to evaluate. The format of this file is described in the
  [official documentation](https://github.com/aymara/CodeGeeX/tree/main/codegeex/benchmark#evaluation).
- `TMP_DIR`: the folder to use as a temporary directory on your computer

Please note that the `prompt` key must also be provided in addition to `task_id` and `generation`.

Please note that you must generate at least 200 solutions for each problem (see the original [paper](https://arxiv.org/abs/2303.17568) for more details).

There is fake results available for you to test the script in `data/fake_generated_files`.

